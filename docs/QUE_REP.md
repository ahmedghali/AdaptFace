# Questions & RÃ©ponses - AdaptFace

> Ce document explique les choix techniques du projet AdaptFace.

---

## 1. C'est quoi DINO et CLIP?

### DINOv2 (Meta AI, 2023)

**DINO** = **Di**stillation with **No** Labels

| Aspect | Description |
|--------|-------------|
| **CrÃ©ateur** | Meta AI (Facebook) |
| **Type** | Self-supervised Vision Transformer |
| **EntraÃ®nement** | 142M images sans labels (auto-supervisÃ©) |
| **Architecture** | ViT-S/14 (Small, patch 14x14) |
| **ParamÃ¨tres** | 22M |
| **Force** | Excellentes features visuelles gÃ©nÃ©rales |

#### Qu'est-ce que "Self-supervised" (Auto-supervisÃ©)?

**OUI, tu as bien compris!** Le modÃ¨le s'entraÃ®ne SANS labels d'identitÃ©.

```
EntraÃ®nement DINO:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image originale â†’ [Augmentation 1] â†’ Vue 1                â”‚
â”‚                  â†’ [Augmentation 2] â†’ Vue 2                â”‚
â”‚                                                             â”‚
â”‚  Objectif: Vue 1 et Vue 2 doivent avoir des features       â”‚
â”‚            SIMILAIRES (car c'est la mÃªme image!)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Le modÃ¨le apprend: "Quelles caractÃ©ristiques sont STABLES
malgrÃ© les transformations (rotation, crop, couleur...)?"
```

**RÃ©sultat**: DINO apprend Ã  extraire des features visuelles robustes (contours, textures, formes) SANS savoir ce qu'est un visage, un chat, etc.

#### Features "Visuelles pures" - Qu'est-ce que Ã§a veut dire?

```
DINO extrait des features de BAS NIVEAU:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Visage â†’ DINO â†’ [0.2, 0.8, 0.1, ...]  â”‚
â”‚                                         â”‚
â”‚  Ces nombres reprÃ©sentent:              â”‚
â”‚  - Contours du nez                      â”‚
â”‚  - Texture de la peau                   â”‚
â”‚  - Distance entre les yeux             â”‚
â”‚  - Forme des sourcils                   â”‚
â”‚  - SymÃ©trie du visage                   â”‚
â”‚                                         â”‚
â”‚  = CaractÃ©ristiques GÃ‰OMÃ‰TRIQUES        â”‚
â”‚    et VISUELLES directes                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pourquoi pour la reconnaissance faciale?**
- DINO a appris sur 142M d'images diverses
- Il sait extraire des caractÃ©ristiques visuelles stables
- On ajoute LoRA/DA-LoRA pour le spÃ©cialiser sur les visages

---

### CLIP (OpenAI, 2021)

**CLIP** = **C**ontrastive **L**anguage-**I**mage **P**re-training

| Aspect | Description |
|--------|-------------|
| **CrÃ©ateur** | OpenAI |
| **Type** | Vision-Language Transformer |
| **EntraÃ®nement** | 400M paires image-texte (contrastif) |
| **Architecture** | ViT-B/16 (Base, patch 16x16) |
| **ParamÃ¨tres** | 86M (vision encoder) |
| **Force** | Features robustes aux variations |

#### Qu'est-ce que "Contrastif"?

```
EntraÃ®nement CLIP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  Paires POSITIVES (doivent Ãªtre proches):                   â”‚
â”‚  [Photo de chat] â†â†’ "A photo of a cat"     âœ“ Match!        â”‚
â”‚                                                             â”‚
â”‚  Paires NÃ‰GATIVES (doivent Ãªtre Ã©loignÃ©es):                 â”‚
â”‚  [Photo de chat] â†â†’ "A photo of a dog"     âœ— Pas match!    â”‚
â”‚  [Photo de chat] â†â†’ "A red car"            âœ— Pas match!    â”‚
â”‚                                                             â”‚
â”‚  Objectif: Rapprocher image-texte qui correspondent,        â”‚
â”‚            Ã‰loigner ceux qui ne correspondent pas           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**IMPORTANT pour notre projet**: On utilise SEULEMENT la partie vision de CLIP (pas le texte!). On jette le text encoder.

#### Features "SÃ©mantiques" - Qu'est-ce que Ã§a veut dire?

```
CLIP extrait des features de HAUT NIVEAU:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Visage â†’ CLIP â†’ [0.2, 0.8, 0.1, ...]  â”‚
â”‚                                         â”‚
â”‚  Ces nombres reprÃ©sentent:              â”‚
â”‚  - "Personne Ã¢gÃ©e" vs "Jeune"          â”‚
â”‚  - "Expression souriante"               â”‚
â”‚  - "Visage de profil"                   â”‚
â”‚  - "Ã‰clairage studio"                   â”‚
â”‚                                         â”‚
â”‚  = Concepts de HAUT NIVEAU              â”‚
â”‚    (car entraÃ®nÃ© avec du texte)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Pourquoi CLIP pour la reconnaissance faciale?

**ATTENTION**: On n'utilise PAS le texte pour reconnaÃ®tre! Voici pourquoi CLIP peut aider:

```
Le vision encoder de CLIP a appris des features ROBUSTES:
- Il a vu "young woman smiling" et "old man serious"
- Donc il a appris Ã  distinguer Ã¢ge, expression, pose
- Ces features peuvent aider pour les cas difficiles

Pour nous:
- On prend JUSTE le vision encoder
- On ajoute LoRA pour le fine-tuner sur les visages
- On utilise ses features robustes, PAS le texte
```

---

## 2. C'est quoi LoRA?

**LoRA** = **Lo**w-**R**ank **A**daptation (Microsoft, 2021)

### Le problÃ¨me que LoRA rÃ©sout

```
Fine-tuning COMPLET (mÃ©thode classique):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DINO a 22 millions de paramÃ¨tres                          â”‚
â”‚  â†’ Il faut stocker 22M de gradients en mÃ©moire             â”‚
â”‚  â†’ Il faut modifier 22M de poids                           â”‚
â”‚  â†’ TRÃˆS COÃ›TEUX en mÃ©moire GPU!                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### L'idÃ©e de LoRA: Factorisation matricielle

**Question**: Pourquoi LoRA n'a pas la mÃªme taille que W?

**RÃ©ponse**: GrÃ¢ce Ã  la factorisation LOW-RANK (rang faible)!

```
Prenons une couche linÃ©aire de DINO:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  W = matrice [384 Ã— 384] = 147,456 paramÃ¨tres              â”‚
â”‚                                                             â”‚
â”‚  Fine-tuning classique:                                     â”‚
â”‚  Î”W = matrice [384 Ã— 384] = 147,456 paramÃ¨tres Ã  entraÃ®ner â”‚
â”‚                                                             â”‚
â”‚  LoRA avec rank=16:                                         â”‚
â”‚  A = matrice [16 Ã— 384]  = 6,144 paramÃ¨tres                â”‚
â”‚  B = matrice [384 Ã— 16]  = 6,144 paramÃ¨tres                â”‚
â”‚  Total LoRA = 12,288 paramÃ¨tres (8% de W!)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visualisation des dimensions

```
                    Fine-tuning classique
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                 â”‚
                    â”‚   Î”W [384Ã—384]  â”‚  = 147,456 params
                    â”‚                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    LoRA (rank=16)
            â”Œâ”€â”€â”€â”
            â”‚   â”‚  B [384Ã—16]
            â”‚   â”‚  = 6,144 params
            â”‚   â”‚
            â””â”€â”€â”€â”˜
               Ã—
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  A [16Ã—384]     â”‚ = 6,144 params
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  B Ã— A [384Ã—384]â”‚  = MÃªme forme que W!
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total LoRA: 6,144 + 6,144 = 12,288 params (vs 147,456)
```

### L'Ã©quation expliquÃ©e

```
W' = W + B Ã— A

OÃ¹:
- W  = poids originaux [384 Ã— 384] â†’ GELÃ‰S (on ne touche pas!)
- A  = petite matrice  [16 Ã— 384]  â†’ ENTRAÃNABLE
- B  = petite matrice  [384 Ã— 16]  â†’ ENTRAÃNABLE
- B Ã— A = produit matriciel [384 Ã— 384] â†’ MÃªme taille que W!

Le produit B Ã— A donne une matrice de MÃŠME TAILLE que W,
mais on n'entraÃ®ne que A et B (beaucoup plus petits).
```

### D'oÃ¹ vient le "1-5%" de paramÃ¨tres?

**Calcul pour DINOv2 ViT-S/14:**

```
DINO a 12 blocs Transformer, chaque bloc a:
- qkv (query, key, value): [384 Ã— 1152] = 442,368 params
- proj (projection):       [384 Ã— 384]  = 147,456 params

On applique LoRA Ã  qkv et proj dans chaque bloc:

LoRA pour qkv (rank=16):
- A: [16 Ã— 384] = 6,144
- B: [1152 Ã— 16] = 18,432
- Total: 24,576 params par bloc

LoRA pour proj (rank=16):
- A: [16 Ã— 384] = 6,144
- B: [384 Ã— 16] = 6,144
- Total: 12,288 params par bloc

Total LoRA par bloc: 24,576 + 12,288 = 36,864
Total LoRA (12 blocs): 36,864 Ã— 12 = 442,368 params

Pourcentage: 442,368 / 22,000,000 = 2.0%
```

**Donc ~2% des paramÃ¨tres sont entraÃ®nÃ©s, pas 100%!**

### RÃ©sumÃ© LoRA

| Aspect | Fine-tuning complet | LoRA rank=16 |
|--------|---------------------|--------------|
| Params entraÃ®nÃ©s | 22M (100%) | ~442K (2%) |
| MÃ©moire GPU | ~8 GB | ~2 GB |
| Backbone modifiÃ© | Oui | Non (gelÃ©) |
| Temps entraÃ®nement | Lent | Rapide |

---

## 3. C'est quoi DA-LoRA? (Notre contribution!)

**DA-LoRA** = **D**omain-**A**ware **LoRA**

### ProblÃ¨me que Ã§a rÃ©sout

Le LoRA standard utilise UNE SEULE adaptation pour TOUTES les images. Mais les visages varient selon:
- **Pose**: frontal, profil, 3/4
- **Ã‚ge**: jeune, adulte, Ã¢gÃ©
- **Ã‰clairage**: studio, extÃ©rieur, nuit

### Notre solution: Multiple LoRA + Domain Classifier

```
Standard LoRA:   W' = W + B Ã— A                    (1 seule adaptation)
DA-LoRA:         W' = W + Î£(wâ‚– Ã— Bâ‚– Ã— Aâ‚–)          (K adaptations pondÃ©rÃ©es)

OÃ¹:
- K = nombre de domaines (3 dans notre cas)
- wâ‚– = poids du domaine k (prÃ©dit par le classifier)
- (Aâ‚–, Bâ‚–) = matrices LoRA spÃ©cifiques au domaine k
```

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DA-LoRA                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Image â†’ Backbone â†’ Features â†’ Domain Classifier            â”‚
â”‚              â”‚                      â”‚                       â”‚
â”‚              â”‚                      â–¼                       â”‚
â”‚              â”‚               [wâ‚, wâ‚‚, wâ‚ƒ]                   â”‚
â”‚              â”‚                      â”‚                       â”‚
â”‚              â–¼                      â–¼                       â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â”‚  wâ‚ Ã— (Bâ‚ Ã— Aâ‚)  â† Domaine 1    â”‚                 â”‚
â”‚         â”‚+ wâ‚‚ Ã— (Bâ‚‚ Ã— Aâ‚‚)  â† Domaine 2    â”‚                 â”‚
â”‚         â”‚+ wâ‚ƒ Ã— (Bâ‚ƒ Ã— Aâ‚ƒ)  â† Domaine 3    â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                      â”‚                                      â”‚
â”‚                      â–¼                                      â”‚
â”‚              Embedding final                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Avantages de DA-LoRA

| Avantage | Explication |
|----------|-------------|
| **SpÃ©cialisation** | Chaque domaine a son adaptation |
| **Dynamique** | Poids calculÃ©s pour chaque image |
| **Robustesse** | Meilleur sur pose/Ã¢ge difficiles |
| **InterprÃ©table** | On voit quel domaine est activÃ© |

---

## 4. Pourquoi DINO et CLIP comme backbones?

### Comparaison

| CritÃ¨re | DINOv2 | CLIP |
|---------|--------|------|
| **EntraÃ®nement original** | Images seules | Images + texte |
| **Features extraites** | GÃ©omÃ©trie, texture | Attributs, variations |
| **Taille modÃ¨le** | 22M (petit, rapide) | 86M (moyen, plus lent) |
| **Force pour visages** | DÃ©tails fins du visage | Robustesse aux variations |

### Pourquoi ces deux-lÃ ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nos critÃ¨res de sÃ©lection:                                 â”‚
â”‚                                                             â”‚
â”‚  1. PrÃ©-entraÃ®nÃ©s sur BEAUCOUP de donnÃ©es (pas de scratch) â”‚
â”‚  2. Architecture ViT (Vision Transformer) - Ã©tat de l'art  â”‚
â”‚  3. Features de qualitÃ© pour le fine-tuning                â”‚
â”‚  4. Compatible avec notre GPU (pas trop gros)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **DINOv2 (choix principal)**
   - 22M paramÃ¨tres = rapide Ã  entraÃ®ner
   - Excellentes features visuelles
   - PrÃ©-entraÃ®nÃ© sur 142M images
   - IdÃ©al pour notre RTX 3060/4070

2. **CLIP (alternative pour comparaison)**
   - Approche diffÃ©rente (vision + langage)
   - Peut capturer des attributs diffÃ©rents
   - Test si Ã§a amÃ©liore les cas difficiles (Ã¢ge, pose)

### Ce qu'on NE choisit PAS

| Backbone | Pourquoi pas? |
|----------|---------------|
| ResNet-50 | Architecture ancienne (2015), moins bon que ViT |
| VGGFace | SpÃ©cifique visages mais architecture dÃ©passÃ©e |
| EntraÃ®nement from scratch | Trop long, pas assez de donnÃ©es |

---

## 5. Pourquoi pas CLIP + DA-LoRA?

**Excellente question!** On PEUT et on DEVRAIT le faire!

### Plan d'expÃ©riences complet

| Exp | Backbone | Adaptation | PrioritÃ© | Status |
|-----|----------|------------|----------|--------|
| EXP-001 | DINOv2 | LoRA | Baseline | âœ… TerminÃ© |
| EXP-002 | CLIP | LoRA | Alternative | â³ Ã€ faire |
| **EXP-003** | **DINOv2** | **DA-LoRA** | **Principal** | ğŸ”„ En cours |
| **EXP-004** | **CLIP** | **DA-LoRA** | **Nouveau!** | â³ Ã€ planifier |

### Pourquoi j'ai dit seulement CLIP + LoRA?

C'Ã©tait une simplification. La matrice complÃ¨te des expÃ©riences est:

```
                    LoRA          DA-LoRA
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    DINOv2    â”‚  EXP-001    â”‚  EXP-003    â”‚
              â”‚  (baseline) â”‚  (principal)â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    CLIP      â”‚  EXP-002    â”‚  EXP-004    â”‚
              â”‚(alternative)â”‚  (nouveau!) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ordre recommandÃ©

1. **EXP-001** âœ… DINOv2 + LoRA (baseline fait)
2. **EXP-003** ğŸ”„ DINOv2 + DA-LoRA (en cours)
3. **EXP-004** â³ CLIP + DA-LoRA (aprÃ¨s EXP-003)
4. **EXP-002** â³ CLIP + LoRA (optionnel, pour comparaison)

### Commande pour CLIP + DA-LoRA

```bash
python train.py --backbone clip --use-dalora --num-domains 3 --batch-size 64 --epochs 40 --wandb
```

---

## RÃ©sumÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AdaptFace Architecture                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚   Backbones (features de base):                            â”‚
â”‚   â”œâ”€â”€ DINOv2: Auto-supervisÃ©, features visuelles          â”‚
â”‚   â””â”€â”€ CLIP: Vision-langage, features sÃ©mantiques          â”‚
â”‚                                                            â”‚
â”‚   Adaptations (fine-tuning efficace):                      â”‚
â”‚   â”œâ”€â”€ LoRA: Une adaptation pour tout                       â”‚
â”‚   â””â”€â”€ DA-LoRA: Adaptations spÃ©cifiques par domaine        â”‚
â”‚                                                            â”‚
â”‚   Notre contribution = DA-LoRA                             â”‚
â”‚   â†’ Meilleur sur pose/Ã¢ge difficiles                       â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Document crÃ©Ã© le 2025-12-22*
